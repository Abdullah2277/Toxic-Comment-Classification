# Toxic-Comment-Classification
Online hate speech has emerged as a major concern in the contemporary digital universe. With thousands of users commenting, chatting, and discussing asynchronously, it is really hard to keep the guidelines and moderation systems effective. There is an immense volume of online communication and thus, manual moderation is not an option for the majority of cases. The advanced techniques of NLP and machine learning in the form of automated detection systems can really add to the content moderation process, making it more effective and just.

Nonetheless, detecting hurtful comments is a difficult task for NLP. The presence of such issues as irony, informal speech, hidden words, and discrimination against non-dominant groups all contribute to the decline of the model's performance. Moreover, sometimes, offensive comments are not easy to classify and can be assigned to more than one category — a single comment may be regarded as an insult and, at the same time, as a hate message based on identity.

This project is aimed at the creation of an NLP-based system that would automatically detect and classify toxic online comments. The user-generated comments in the dataset are labeled by six overlapping categories—toxic, severe toxic, obscene, threat, insult, and identity hate; so the problem can be considered a multi-label text classification issue. This project has a dual academic and ethical rationale. From the academic side, it studies to what extent NLP methods like tokenization, lemmatization, TF-IDF vectorization, and explainable AI techniques (e.g., SHAP) can promote model transparency and accuracy. From the ethical side, it strives to foster healthy online communication by supplying tools that will automatically flag and neutralize offensive language.
