{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxNum_OXQpgL"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, roc_curve, auc, classification_report\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXFQ0DkrQr4p"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Load and Prepare Dataset\n",
        "# -----------------------\n",
        "df = pd.read_csv(\"train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V54iNRYeQz7Z"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVvUJ9liQzJX"
      },
      "outputs": [],
      "source": [
        "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "df = df.dropna(subset=['comment_text'])\n",
        "df[label_cols] = df[label_cols].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtO492kBQ2zm"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWZysEupQ8dr"
      },
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUs8reUyRByD"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH_jY6QGRF7i"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Tokenization\n",
        "# -----------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3utx3Qf7ROwz"
      },
      "outputs": [],
      "source": [
        "# Convert labels to float for BCE loss\n",
        "train_dataset = train_dataset.map(lambda x: {\"labels\": [float(x[col]) for col in label_cols]})\n",
        "test_dataset = test_dataset.map(lambda x: {\"labels\": [float(x[col]) for col in label_cols]})\n",
        "\n",
        "train_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "test_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsVFxDcwRTiX"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Model\n",
        "# -----------------------\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=len(label_cols),\n",
        "    problem_type=\"multi_label_classification\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-9wGp5MRWCN"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Metrics\n",
        "# -----------------------\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    probs = 1 / (1 + np.exp(-logits))\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "    f1 = f1_score(labels, preds, average=\"macro\")\n",
        "    roc_auc = roc_auc_score(labels, probs, average=\"macro\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"f1\": f1, \"roc_auc\": roc_auc, \"accuracy\": acc}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-N6QOA9hRaiS"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Training Arguments\n",
        "# -----------------------\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    eval_strategy=\"epoch\",        # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",              # Save checkpoint each epoch\n",
        "    learning_rate=3e-5,                 # Reasonable learning rate for DistilBERT\n",
        "    per_device_train_batch_size=16,     # Smaller batch size for faster iteration\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,                 # Reduce epochs for quick training\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,                   # Log less frequently\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",         # Use F1 to track best model\n",
        "    greater_is_better=True,\n",
        "    fp16=True,                          # Use mixed precision (if GPU supports it)\n",
        "    report_to=\"none\",                   # Disable W&B or other tracking\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znkpy12CRcwl"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Trainer\n",
        "# -----------------------\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f08OJhhNR3AG"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Evaluation and Predictions\n",
        "# -----------------------\n",
        "results = trainer.evaluate()\n",
        "print(\"\\nðŸ“Š Evaluation Results:\", results)\n",
        "\n",
        "# Predict probabilities and binary labels\n",
        "predictions = trainer.predict(test_dataset)\n",
        "logits = predictions.predictions\n",
        "probs = 1 / (1 + np.exp(-logits))\n",
        "binary_preds = (probs >= 0.5).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YuiiebGvQPpk"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Plot ROC Curves\n",
        "# -----------------------\n",
        "for i, label in enumerate(label_cols):\n",
        "    fpr, tpr, _ = roc_curve(predictions.label_ids[:, i], probs[:, i])\n",
        "    plt.plot(fpr, tpr, label=f\"{label} (AUC={auc(fpr, tpr):.3f})\")\n",
        "\n",
        "plt.plot([0, 1], [0, 1], \"k--\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"ROC Curves - Multi-Label Toxic Comment Classification\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEFnq94jyN8e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "# Get true and predicted labels\n",
        "y_true = predictions.label_ids\n",
        "y_probs = probs\n",
        "y_pred = binary_preds\n",
        "\n",
        "label_names = label_cols\n",
        "# ðŸ”¹ Detailed classification report (Precision, Recall, F1)\n",
        "report = classification_report(y_true, y_pred, target_names=label_names, output_dict=True)\n",
        "print(\"Classification Report (Per Label):\")\n",
        "for label in label_names:\n",
        "    print(f\"{label:15s} | Precision: {report[label]['precision']:.3f} | \"\n",
        "          f\"Recall: {report[label]['recall']:.3f} | \"\n",
        "          f\"F1-score: {report[label]['f1-score']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omqqsBltzYn2"
      },
      "outputs": [],
      "source": [
        "# ðŸ”¹ AUROC for each label\n",
        "print(\"\\nAUROC Scores (Per Label):\")\n",
        "for i, label in enumerate(label_names):\n",
        "    auc_score = roc_auc_score(y_true[:, i], y_probs[:, i])\n",
        "    print(f\"{label:15s} | AUC: {auc_score:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aMcU_A38z7Hg"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# ðŸ”® Real-world Prediction\n",
        "# -----------------------\n",
        "def predict_text(text):\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Get model outputs\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "\n",
        "    # Binary predictions (threshold 0.5)\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "    # Prepare readable output\n",
        "    results = {label_cols[i]: float(probs[i]) for i in range(len(label_cols))}\n",
        "    binary_results = {label_cols[i]: int(preds[i]) for i in range(len(label_cols))}\n",
        "\n",
        "    return results, binary_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3OvwoWD1hwH"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "sample_text = \"You are such an idiot, stop posting nonsense!\"\n",
        "predict_text(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zl6nm5Ln2IZE"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Load your test dataset\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Prepare lists to store results\n",
        "all_probs = []\n",
        "all_preds = []\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Disable gradient calculation for faster inference\n",
        "with torch.no_grad():\n",
        "    for text in tqdm(test_df[\"comment_text\"], desc=\"Predicting\"):\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "        preds = (probs >= 0.5).astype(int)\n",
        "\n",
        "        all_probs.append(probs)\n",
        "        all_preds.append(preds)\n",
        "\n",
        "# Convert to DataFrames\n",
        "prob_df = pd.DataFrame(all_probs, columns=[f\"{col}_prob\" for col in label_cols])\n",
        "pred_df = pd.DataFrame(all_preds, columns=label_cols)\n",
        "\n",
        "# Combine with IDs and comments\n",
        "final_df = pd.concat([test_df[[\"id\", \"comment_text\"]].reset_index(drop=True), pred_df], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6TfIIsk2PEI"
      },
      "outputs": [],
      "source": [
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rvw_Tv7k7LP0"
      },
      "outputs": [],
      "source": [
        "final_df.to_csv('test_result.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eeabae7"
      },
      "outputs": [],
      "source": [
        "# -----------------------\n",
        "# Save Model\n",
        "# -----------------------\n",
        "# Save the model\n",
        "trainer.save_model(\"./toxic_comment_model\")\n",
        "\n",
        "print(\"Model saved successfully to ./toxic_comment_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1e8fa42"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Define the directory to be zipped and the name of the zip file\n",
        "directory_to_zip = \"./toxic_comment_model\"\n",
        "zip_file_name = \"toxic_comment_model.zip\"\n",
        "\n",
        "# Create the zip file\n",
        "shutil.make_archive(zip_file_name.replace('.zip', ''), 'zip', directory_to_zip)\n",
        "\n",
        "print(f\"'{directory_to_zip}' has been zipped to '{zip_file_name}'. You can now download this file from the file browser.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsBghky27ddj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}